{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regressions have limitations.\n",
    "\n",
    "As it stands, the algorithm could generate a prediction *anywhere on the real number line*. This *may* be realistic, like if I'm predicting national surpluses/debts.\n",
    "\n",
    "But what if I'm predicting values of a variable that doesn't take, say, negative values, like temperature in Kelvin?\n",
    "\n",
    "What if I'm predicting values of a variable that takes only integer values, like the number of mouseclicks on my killer ds blog per minute?\n",
    "\n",
    "What if I'm predicting probabilities? Or something Boolean / Bernoullian?\n",
    "\n",
    "What if my the shape of my errors changes as a function of the value of the dependent variable?\n",
    "\n",
    "Am I stuck using linear regression? There's got to be a better way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy now is to *generalize* the notion of linear regression; regression will become a special case. In particular, we'll keep the idea of the regression best-fit line, but now **we'll allow the model to be constructed from the dependent variable through some (non-trivial) function of the linear predictor**. This function is standardly called the **link function**.\n",
    "\n",
    "Let's say we've constructed our best-fit line, i.e. our linear predictor, $\\hat{L} = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia has a very helpful page about generalized linear models! <br/> Access it here: https://en.wikipedia.org/wiki/Generalized_linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Consider the following transformation: <br/>\n",
    "$\\large\\hat{y} = \\Large\\frac{1}{1 + e^{-\\hat{L}}} \\large= \\Large\\frac{1}{1 + e^{-\\beta_0 + ... + \\beta_nx_n}}$. This is called the **sigmoid function**.\n",
    "\n",
    "We're imagining that $\\hat{L}$ can take any values between $-\\infty$ and $\\infty$.\n",
    "\n",
    "$\\large\\rightarrow$ But what values can $\\hat{y}$ take? What does this function even look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's plot this function here:\n",
    "\n",
    "X = np.linspace(-30, 30, 300)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('$\\\\frac{1}{1 + e^{-X}}$')\n",
    "plt.plot(X, 1 / (1 + np.exp(-X)), 'r', lw=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we fit a line to our dependent variable if its values are already stored as probabilities? We can use the inverse of the sigmoid function, and just set our regression equation equal to that. The inverse of the sigmoid function is called the **logit function**, and it looks like this:\n",
    "\n",
    "$\\large f(y) = \\ln\\left(\\frac{y}{1 - y}\\right)$. Notice that the domain of this function is $(0, 1)$.\n",
    "\n",
    "$\\hspace{110mm}$(Quick proof that logit and sigmoid are inverse functions:\n",
    "\n",
    "$\\hspace{170mm}x = \\frac{1}{1 + e^{-y}}$; <br/>\n",
    "$\\hspace{170mm}$so $1 + e^{-y} = \\frac{1}{x}$; <br/>\n",
    "$\\hspace{170mm}$so $e^{-y} = \\frac{1 - x}{x}$; <br/>\n",
    "$\\hspace{170mm}$so $-y = \\ln\\left(\\frac{1 - x}{x}\\right)$; <br/>\n",
    "$\\hspace{170mm}$so $y = \\ln\\left(\\frac{x}{1 - x}\\right)$.)\n",
    "\n",
    "Our regression equation will now look like this:\n",
    "\n",
    "$\\large\\ln\\left(\\frac{y}{1 - y}\\right) = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$.\n",
    "\n",
    "This equation is used for a **logistic regression**: Its characteristic link function is this logit function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other ways to squeeze the results of a linear regression into the set (0, 1).\n",
    "\n",
    "But *this* function represents the **log-odds** of success (y = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression in Sci-kit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data = pd.DataFrame([[4.0, 650, 750, 1], [3.0, 780, 780, 1],\n",
    "                     [2.0, 570, 600, 0], [2.5, 780, 770, 1],\n",
    "                    [3.8, 600, 550, 0], [1.5, 600, 650, 0]],\n",
    "                    columns=['gpa', 'sat_v', 'sat_q', 'admitted'])\n",
    "\n",
    "\n",
    "X = data.drop('admitted', axis=1)\n",
    "y = data['admitted']\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a logistic regression object with the 'liblinear' solver,\n",
    "# which is good for small datasets.\n",
    "\n",
    "\n",
    "\n",
    "# Now fit it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now predict admission for someone with a 4.0 GPA, a 600 Verbal score\n",
    "# and a 700 quantitative score!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also predict probabilities for the two classes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson Regression\n",
    "\n",
    "Here's a different sort of regression equation:\n",
    "\n",
    "$\\large\\ln(y) = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$. The link function is simply $\\ln(y)$ and so we have:\n",
    "\n",
    "$\\large\\hat{y} = e^\\hat{L} = e^{\\beta_0 + ... + \\beta_nx_n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The domain, or \"support\", for a Poisson distribution is {0, 1, 2, ... }. Can you see why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson Regression in Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards = pd.read_csv('https://stats.idre.ucla.edu/stat/data/poisson_sim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#awards.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is this dataset about?\n",
    "\n",
    "The data show the number of awards earned by students at one high school. 'Prog' is a coded version of the sort of program in which the student was enrolled and 'math' is a score on a math exam.\n",
    "\n",
    "Let's one-hot encode it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll just use pd.get_dummies()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a statsmodels summary here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreting the results\n",
    "\n",
    "np.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
